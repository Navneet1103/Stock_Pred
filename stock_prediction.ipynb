{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import pandas as pd\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense,LSTM,Dropout,BatchNormalization\r\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\r\n",
    "from sklearn.preprocessing import scale\r\n",
    "from collections import deque\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "\r\n",
    "# import plotly.graph_objects as go\r\n",
    "# from plotly.subplots import make_subplots\r\n",
    "# import plotly.io as pio\r\n",
    "# pio.renderers.default = 'jupyterlab'\r\n",
    "# import plotly.offline as po"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I have used dataset from KAGGLE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Importing and Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv(r'D:\\Data Analytics for Business\\Keras learning\\NSE Coal India\\BAJAJFINSV__EQ__NSE__NSE__MINUTE.csv')\r\n",
    "data.head().T"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   0                          1  \\\n",
       "timestamp  2017-01-02 09:15:00+05:30  2017-01-02 09:16:00+05:30   \n",
       "open                          2898.0                     2901.4   \n",
       "high                         2901.45                     2913.0   \n",
       "low                           2897.0                     2895.1   \n",
       "close                         2901.4                    2899.25   \n",
       "volume                         347.0                     1419.0   \n",
       "\n",
       "                                   2                          3  \\\n",
       "timestamp  2017-01-02 09:17:00+05:30  2017-01-02 09:18:00+05:30   \n",
       "open                         2900.85                    2871.65   \n",
       "high                         2900.85                     2885.0   \n",
       "low                           2864.2                     2867.0   \n",
       "close                         2868.3                     2885.0   \n",
       "volume                         642.0                      712.0   \n",
       "\n",
       "                                   4  \n",
       "timestamp  2017-01-02 09:19:00+05:30  \n",
       "open                          2885.0  \n",
       "high                          2895.0  \n",
       "low                          2884.95  \n",
       "close                         2890.0  \n",
       "volume                         637.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>2017-01-02 09:15:00+05:30</td>\n",
       "      <td>2017-01-02 09:16:00+05:30</td>\n",
       "      <td>2017-01-02 09:17:00+05:30</td>\n",
       "      <td>2017-01-02 09:18:00+05:30</td>\n",
       "      <td>2017-01-02 09:19:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>2898.0</td>\n",
       "      <td>2901.4</td>\n",
       "      <td>2900.85</td>\n",
       "      <td>2871.65</td>\n",
       "      <td>2885.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>2901.45</td>\n",
       "      <td>2913.0</td>\n",
       "      <td>2900.85</td>\n",
       "      <td>2885.0</td>\n",
       "      <td>2895.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>2897.0</td>\n",
       "      <td>2895.1</td>\n",
       "      <td>2864.2</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>2884.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>2901.4</td>\n",
       "      <td>2899.25</td>\n",
       "      <td>2868.3</td>\n",
       "      <td>2885.0</td>\n",
       "      <td>2890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>347.0</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "seq_length = 80                     # how many sequence should we use to predict\r\n",
    "future_predict_period =2            # based on sequence data we are trying to predict for next 2 periods"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                open           high            low          close  \\\n",
       "count  370408.000000  370408.000000  370408.000000  370408.000000   \n",
       "mean     6197.359667    6200.971033    6193.591429    6197.316345   \n",
       "std      1615.999775    1616.535168    1615.381194    1615.985123   \n",
       "min      2871.650000    2875.000000    2864.200000    2868.300000   \n",
       "25%      5069.500000    5072.050000    5065.550000    5069.350000   \n",
       "50%      5964.000000    5968.200000    5960.000000    5964.000000   \n",
       "75%      7169.062500    7173.000000    7164.312500    7169.012500   \n",
       "max      9925.550000    9950.000000    9925.000000    9925.100000   \n",
       "\n",
       "              volume  \n",
       "count  370408.000000  \n",
       "mean      976.804375  \n",
       "std      2107.229357  \n",
       "min         0.000000  \n",
       "25%       123.000000  \n",
       "50%       352.000000  \n",
       "75%       982.000000  \n",
       "max    133328.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>370408.000000</td>\n",
       "      <td>370408.000000</td>\n",
       "      <td>370408.000000</td>\n",
       "      <td>370408.000000</td>\n",
       "      <td>370408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6197.359667</td>\n",
       "      <td>6200.971033</td>\n",
       "      <td>6193.591429</td>\n",
       "      <td>6197.316345</td>\n",
       "      <td>976.804375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1615.999775</td>\n",
       "      <td>1616.535168</td>\n",
       "      <td>1615.381194</td>\n",
       "      <td>1615.985123</td>\n",
       "      <td>2107.229357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2871.650000</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2864.200000</td>\n",
       "      <td>2868.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5069.500000</td>\n",
       "      <td>5072.050000</td>\n",
       "      <td>5065.550000</td>\n",
       "      <td>5069.350000</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5964.000000</td>\n",
       "      <td>5968.200000</td>\n",
       "      <td>5960.000000</td>\n",
       "      <td>5964.000000</td>\n",
       "      <td>352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7169.062500</td>\n",
       "      <td>7173.000000</td>\n",
       "      <td>7164.312500</td>\n",
       "      <td>7169.012500</td>\n",
       "      <td>982.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9925.550000</td>\n",
       "      <td>9950.000000</td>\n",
       "      <td>9925.000000</td>\n",
       "      <td>9925.100000</td>\n",
       "      <td>133328.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Timestamp is a string and has hours and minutes so we need to remove them,then convert it to a datetime and put in the index.**\r\n",
    "\r\n",
    "**We only need close and volume so need to remove everything apart from these two.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data['timestamp'] =[ n[:10] for n in data['timestamp']]\r\n",
    "for c in data.columns:\r\n",
    "    if c != 'timestamp':\r\n",
    "        data[c]  = data[c].astype('float')\r\n",
    "data['date'] =pd.to_datetime(data['timestamp'],format='%Y-%m-%d')\r\n",
    "data = data.set_index(data['date'])\r\n",
    "\r\n",
    "data = data.drop(['open','high','low','date','timestamp'],1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-b6a93e7b59f0>:8: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop(['open','high','low','date','timestamp'],1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              close  volume\n",
       "date                       \n",
       "2017-01-02  2901.40   347.0\n",
       "2017-01-02  2899.25  1419.0\n",
       "2017-01-02  2868.30   642.0\n",
       "2017-01-02  2885.00   712.0\n",
       "2017-01-02  2890.00   637.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>2901.40</td>\n",
       "      <td>347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>2899.25</td>\n",
       "      <td>1419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>2868.30</td>\n",
       "      <td>642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>2885.00</td>\n",
       "      <td>712.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>2890.00</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**created a future parameter that is basically a 3 step ahead future of close.**\r\n",
    "\r\n",
    "**I have created it to generate a target that will return 1 if future price will rise and 0 when future price will fall**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "data['future'] = data['close'].shift(-future_predict_period)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def target(current,future):\r\n",
    "    if float(future) > float(current):\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data['target'] = list( map(target,data['close'],data['future']) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**sorted the time index and kept the last 5% of the data as the validation set**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "time = sorted(data.index.values)\r\n",
    "time_5pct = time[-int(0.05*len(time))]\r\n",
    "validation_data = data[(data.index>=time_5pct)]\r\n",
    "training_data = data[(data.index< time_5pct)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Preprocessing function**\r\n",
    "first we need to drop future because otherwise our model will not be able to learn.\r\n",
    "Then i have scaled all the columns.pct_change was generating infinite values so replaced them with na and dropped them.\r\n",
    "I have used sklearn scaler to scale the column.\r\n",
    "\r\n",
    "Created a sequential data of max length 60 using deque and appending data of 60 observations as a 1 observation to the sequencial data.\r\n",
    "Then balanced the no of 0 and 1 in the data and returned the x and y\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def preprocessing(data):\r\n",
    "    data = data.drop('future',1)\r\n",
    "    for col in data.columns:\r\n",
    "        if col !='target':\r\n",
    "            data[col] = data[col].pct_change()\r\n",
    "            data.dropna(inplace=True)\r\n",
    "            data[col].replace(np.inf,np.nan,inplace=True)\r\n",
    "            data[col].dropna(inplace=True)\r\n",
    "            data[col] = scale(data[col].values)\r\n",
    "\r\n",
    "    data.dropna(inplace=True)    \r\n",
    "\r\n",
    "    sequencial_data = []\r\n",
    "    prev_days = deque(maxlen=seq_length)\r\n",
    "\r\n",
    "    for i in data.values:\r\n",
    "        prev_days.append([n for n in i[:-1]])\r\n",
    "        if len(prev_days)==seq_length:\r\n",
    "            sequencial_data.append([np.array(prev_days),i[-1]])\r\n",
    "    random.shuffle(sequencial_data)\r\n",
    "\r\n",
    "    buys = []\r\n",
    "    sells = []\r\n",
    "    for o,v in sequencial_data:\r\n",
    "        if v==0:\r\n",
    "            sells.append([o,v])\r\n",
    "        elif v==1:\r\n",
    "            buys.append([o,v])\r\n",
    "    \r\n",
    "    lower = min(len(sells),len(buys))\r\n",
    "    buys = buys[:lower]\r\n",
    "    sells = sells[:lower]\r\n",
    "    print(len(buys),len(sells))    \r\n",
    "    sequencial_data = buys+sells\r\n",
    "    random.shuffle(sequencial_data)\r\n",
    "    x = []\r\n",
    "    y= []\r\n",
    "\r\n",
    "    for seq,tar in sequencial_data:\r\n",
    "        x.append(seq)\r\n",
    "        y.append(tar)\r\n",
    "\r\n",
    "    return np.array(x),y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x_train,y_train = preprocessing(training_data)\r\n",
    "x_test,y_test = preprocessing(validation_data)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-11-a822a9edc9a7>:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop('future',1)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "168155 168155\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-11-a822a9edc9a7>:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop('future',1)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9149 9149\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "x_train = np.asarray(x_train)\r\n",
    "y_train = np.asarray(y_train)\r\n",
    "x_test = np.asarray(x_test)\r\n",
    "y_test = np.asarray(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Creation and Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Created a LSTM model followed by a dense layer.**\r\n",
    "**Using dropout of 0.2**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def give_model(neurons=128,opt='adam',dense=64):\r\n",
    "    model = Sequential()\r\n",
    "    model.add(LSTM(neurons,input_shape=(x_train.shape[1:]),return_sequences=True,activation='tanh'))\r\n",
    "    model.add(Dropout(0.2))\r\n",
    "    model.add(BatchNormalization())\r\n",
    "\r\n",
    "    model.add(LSTM(neurons,input_shape=(x_train.shape[1:]),return_sequences=True,activation='tanh'))\r\n",
    "    model.add(Dropout(0.2))\r\n",
    "    model.add(BatchNormalization())\r\n",
    "\r\n",
    "    model.add(LSTM(neurons,input_shape=(x_train.shape[1:]),return_sequences=True,activation='tanh'))\r\n",
    "    model.add(Dropout(0.2))\r\n",
    "    model.add(BatchNormalization())\r\n",
    "\r\n",
    "    model.add(LSTM(neurons,input_shape=(x_train.shape[1:]),activation='tanh'))\r\n",
    "    model.add(Dropout(0.2))\r\n",
    "    model.add(BatchNormalization())\r\n",
    "\r\n",
    "    model.add(Dense(dense,activation='relu'))\r\n",
    "    model.add(Dropout(0.2))\r\n",
    "    model.add(Dense(2,activation='softmax'))\r\n",
    "    model.compile(optimizer=opt,loss=SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**I have used Adam as an optimizer with initial lr of 0.001 and decay of 1e-6. Used ModelCheckpoint to checkpointing the data and tensorboard to visualise the model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "name = f'{seq_length}_{future_predict_period}_{int(time.time())}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# opt= Adam(learning_rate=0.01)\r\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{name}')\r\n",
    "chkpt = ModelCheckpoint(filepath= r'D:\\Data Analytics for Business\\Keras learning\\NSE Coal India\\best_model.hdf5',monitor='val_accuracy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "cv = KerasRegressor(build_fn=give_model)\r\n",
    "params = {'opt':['adam','sgd'],'batch_size':[64,128],'neurons':[64,128,256],'dense':[32,64,128]}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "randomized_cvv = RandomizedSearchCV(cv,params)\r\n",
    "randomized_cvv.fit(x_train,y_train,epochs=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "randomized_cvv.best_params_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'opt': 'adam', 'neurons': 128, 'dense': 128, 'batch_size': 64}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "model = give_model(neurons=128,dense=128,opt='adam')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "hist = model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=64,epochs=200,callbacks=[tensorboard,chkpt])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "5255/5255 [==============================] - 203s 36ms/step - loss: 0.6964 - accuracy: 0.5237 - val_loss: 0.6904 - val_accuracy: 0.5278\n",
      "Epoch 2/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6904 - accuracy: 0.5354 - val_loss: 0.6914 - val_accuracy: 0.5322\n",
      "Epoch 3/200\n",
      "5255/5255 [==============================] - 187s 36ms/step - loss: 0.6893 - accuracy: 0.5428 - val_loss: 0.6899 - val_accuracy: 0.5393\n",
      "Epoch 4/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6889 - accuracy: 0.5420 - val_loss: 0.6894 - val_accuracy: 0.5344\n",
      "Epoch 5/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6880 - accuracy: 0.5464 - val_loss: 0.6885 - val_accuracy: 0.5394\n",
      "Epoch 6/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6877 - accuracy: 0.5478 - val_loss: 0.6886 - val_accuracy: 0.5345\n",
      "Epoch 7/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6875 - accuracy: 0.5479 - val_loss: 0.6886 - val_accuracy: 0.5397\n",
      "Epoch 8/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6869 - accuracy: 0.5490 - val_loss: 0.6881 - val_accuracy: 0.5400\n",
      "Epoch 9/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6868 - accuracy: 0.5487 - val_loss: 0.6907 - val_accuracy: 0.5314\n",
      "Epoch 10/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6868 - accuracy: 0.5491 - val_loss: 0.6889 - val_accuracy: 0.5375\n",
      "Epoch 11/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6867 - accuracy: 0.5493 - val_loss: 0.6890 - val_accuracy: 0.5327\n",
      "Epoch 12/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6866 - accuracy: 0.5500 - val_loss: 0.6891 - val_accuracy: 0.5414\n",
      "Epoch 13/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6866 - accuracy: 0.5486 - val_loss: 0.6885 - val_accuracy: 0.5383\n",
      "Epoch 14/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6863 - accuracy: 0.5493 - val_loss: 0.6890 - val_accuracy: 0.5336\n",
      "Epoch 15/200\n",
      "5255/5255 [==============================] - 186s 35ms/step - loss: 0.6863 - accuracy: 0.5499 - val_loss: 0.6897 - val_accuracy: 0.5318\n",
      "Epoch 16/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6862 - accuracy: 0.5504 - val_loss: 0.6892 - val_accuracy: 0.5331\n",
      "Epoch 17/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6862 - accuracy: 0.5501 - val_loss: 0.6895 - val_accuracy: 0.5387\n",
      "Epoch 18/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6861 - accuracy: 0.5507 - val_loss: 0.6893 - val_accuracy: 0.5379\n",
      "Epoch 19/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6859 - accuracy: 0.5504 - val_loss: 0.6884 - val_accuracy: 0.5379\n",
      "Epoch 20/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6861 - accuracy: 0.5502 - val_loss: 0.6890 - val_accuracy: 0.5380\n",
      "Epoch 21/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6860 - accuracy: 0.5500 - val_loss: 0.6887 - val_accuracy: 0.5380\n",
      "Epoch 22/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6860 - accuracy: 0.5501 - val_loss: 0.6897 - val_accuracy: 0.5327\n",
      "Epoch 23/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6860 - accuracy: 0.5511 - val_loss: 0.6899 - val_accuracy: 0.5357\n",
      "Epoch 24/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6860 - accuracy: 0.5509 - val_loss: 0.6912 - val_accuracy: 0.5361\n",
      "Epoch 25/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6862 - accuracy: 0.5507 - val_loss: 0.6882 - val_accuracy: 0.5355\n",
      "Epoch 26/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6860 - accuracy: 0.5504 - val_loss: 0.6887 - val_accuracy: 0.5375\n",
      "Epoch 27/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6859 - accuracy: 0.5515 - val_loss: 0.6885 - val_accuracy: 0.5365\n",
      "Epoch 28/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6859 - accuracy: 0.5507 - val_loss: 0.6890 - val_accuracy: 0.5412\n",
      "Epoch 29/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6859 - accuracy: 0.5504 - val_loss: 0.6889 - val_accuracy: 0.5401\n",
      "Epoch 30/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6856 - accuracy: 0.5511 - val_loss: 0.6888 - val_accuracy: 0.5387\n",
      "Epoch 31/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5513 - val_loss: 0.6889 - val_accuracy: 0.5361\n",
      "Epoch 32/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6856 - accuracy: 0.5515 - val_loss: 0.6906 - val_accuracy: 0.5420\n",
      "Epoch 33/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6857 - accuracy: 0.5516 - val_loss: 0.6899 - val_accuracy: 0.5392\n",
      "Epoch 34/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5518 - val_loss: 0.6903 - val_accuracy: 0.5400\n",
      "Epoch 35/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6856 - accuracy: 0.5512 - val_loss: 0.6883 - val_accuracy: 0.5391\n",
      "Epoch 36/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5510 - val_loss: 0.6898 - val_accuracy: 0.5373\n",
      "Epoch 37/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6858 - accuracy: 0.5506 - val_loss: 0.6913 - val_accuracy: 0.5389\n",
      "Epoch 38/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6859 - accuracy: 0.5515 - val_loss: 0.6895 - val_accuracy: 0.5380\n",
      "Epoch 39/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6856 - accuracy: 0.5509 - val_loss: 0.6898 - val_accuracy: 0.5374\n",
      "Epoch 40/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6856 - accuracy: 0.5519 - val_loss: 0.6899 - val_accuracy: 0.5410\n",
      "Epoch 41/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6856 - accuracy: 0.5511 - val_loss: 0.6908 - val_accuracy: 0.5397\n",
      "Epoch 42/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6858 - accuracy: 0.5515 - val_loss: 0.6886 - val_accuracy: 0.5368\n",
      "Epoch 43/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6858 - accuracy: 0.5509 - val_loss: 0.6890 - val_accuracy: 0.5393\n",
      "Epoch 44/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6858 - accuracy: 0.5510 - val_loss: 0.6897 - val_accuracy: 0.5377\n",
      "Epoch 45/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6855 - accuracy: 0.5519 - val_loss: 0.6907 - val_accuracy: 0.5377\n",
      "Epoch 46/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6855 - accuracy: 0.5532 - val_loss: 0.6906 - val_accuracy: 0.5340\n",
      "Epoch 47/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6855 - accuracy: 0.5529 - val_loss: 0.6895 - val_accuracy: 0.5351\n",
      "Epoch 48/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5519 - val_loss: 0.6905 - val_accuracy: 0.5359\n",
      "Epoch 49/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6857 - accuracy: 0.5522 - val_loss: 0.6917 - val_accuracy: 0.5399\n",
      "Epoch 50/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6857 - accuracy: 0.5520 - val_loss: 0.6896 - val_accuracy: 0.5355\n",
      "Epoch 51/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6859 - accuracy: 0.5517 - val_loss: 0.6913 - val_accuracy: 0.5365\n",
      "Epoch 52/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6861 - accuracy: 0.5524 - val_loss: 0.6928 - val_accuracy: 0.5318\n",
      "Epoch 53/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5529 - val_loss: 0.6909 - val_accuracy: 0.5427\n",
      "Epoch 54/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5524 - val_loss: 0.6909 - val_accuracy: 0.5361\n",
      "Epoch 55/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6856 - accuracy: 0.5521 - val_loss: 0.6929 - val_accuracy: 0.5372\n",
      "Epoch 56/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6856 - accuracy: 0.5515 - val_loss: 0.6940 - val_accuracy: 0.5382\n",
      "Epoch 57/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5526 - val_loss: 0.6931 - val_accuracy: 0.5358\n",
      "Epoch 58/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6857 - accuracy: 0.5515 - val_loss: 0.6923 - val_accuracy: 0.5349\n",
      "Epoch 59/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6856 - accuracy: 0.5525 - val_loss: 0.6949 - val_accuracy: 0.5380\n",
      "Epoch 60/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6857 - accuracy: 0.5519 - val_loss: 0.6904 - val_accuracy: 0.5371\n",
      "Epoch 61/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6859 - accuracy: 0.5519 - val_loss: 0.6907 - val_accuracy: 0.5389\n",
      "Epoch 62/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6859 - accuracy: 0.5520 - val_loss: 0.6895 - val_accuracy: 0.5369\n",
      "Epoch 63/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6857 - accuracy: 0.5523 - val_loss: 0.6892 - val_accuracy: 0.5337\n",
      "Epoch 64/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6858 - accuracy: 0.5520 - val_loss: 0.6913 - val_accuracy: 0.5363\n",
      "Epoch 65/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6858 - accuracy: 0.5522 - val_loss: 0.6904 - val_accuracy: 0.5340\n",
      "Epoch 66/200\n",
      "5255/5255 [==============================] - 181s 35ms/step - loss: 0.6857 - accuracy: 0.5518 - val_loss: 0.6931 - val_accuracy: 0.5365\n",
      "Epoch 67/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6860 - accuracy: 0.5508 - val_loss: 0.6898 - val_accuracy: 0.5360\n",
      "Epoch 68/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6859 - accuracy: 0.5506 - val_loss: 0.6920 - val_accuracy: 0.5315\n",
      "Epoch 69/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6861 - accuracy: 0.5502 - val_loss: 0.6897 - val_accuracy: 0.5372\n",
      "Epoch 70/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6860 - accuracy: 0.5510 - val_loss: 0.6916 - val_accuracy: 0.5345\n",
      "Epoch 71/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6861 - accuracy: 0.5510 - val_loss: 0.6899 - val_accuracy: 0.5366\n",
      "Epoch 72/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6862 - accuracy: 0.5496 - val_loss: 0.6905 - val_accuracy: 0.5340\n",
      "Epoch 73/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6861 - accuracy: 0.5497 - val_loss: 0.6915 - val_accuracy: 0.5367\n",
      "Epoch 74/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6870 - accuracy: 0.5477 - val_loss: 0.6922 - val_accuracy: 0.5332\n",
      "Epoch 75/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6866 - accuracy: 0.5486 - val_loss: 0.6897 - val_accuracy: 0.5367\n",
      "Epoch 76/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6865 - accuracy: 0.5495 - val_loss: 0.6915 - val_accuracy: 0.5333\n",
      "Epoch 77/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6866 - accuracy: 0.5494 - val_loss: 0.6931 - val_accuracy: 0.5359\n",
      "Epoch 78/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6861 - accuracy: 0.5509 - val_loss: 0.6896 - val_accuracy: 0.5370\n",
      "Epoch 79/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6864 - accuracy: 0.5496 - val_loss: 0.6884 - val_accuracy: 0.5369\n",
      "Epoch 80/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6862 - accuracy: 0.5495 - val_loss: 0.6890 - val_accuracy: 0.5368\n",
      "Epoch 81/200\n",
      "5255/5255 [==============================] - 188s 36ms/step - loss: 0.6862 - accuracy: 0.5504 - val_loss: 0.6892 - val_accuracy: 0.5377\n",
      "Epoch 82/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6861 - accuracy: 0.5499 - val_loss: 0.6895 - val_accuracy: 0.5375\n",
      "Epoch 83/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6865 - accuracy: 0.5505 - val_loss: 0.6902 - val_accuracy: 0.5414\n",
      "Epoch 84/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6865 - accuracy: 0.5502 - val_loss: 0.6893 - val_accuracy: 0.5399\n",
      "Epoch 85/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6864 - accuracy: 0.5492 - val_loss: 0.6898 - val_accuracy: 0.5368\n",
      "Epoch 86/200\n",
      "5255/5255 [==============================] - 182s 35ms/step - loss: 0.6864 - accuracy: 0.5489 - val_loss: 0.6894 - val_accuracy: 0.5365\n",
      "Epoch 87/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6862 - accuracy: 0.5497 - val_loss: 0.6922 - val_accuracy: 0.5369\n",
      "Epoch 88/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6861 - accuracy: 0.5506 - val_loss: 0.6920 - val_accuracy: 0.5334\n",
      "Epoch 89/200\n",
      "5255/5255 [==============================] - 180s 34ms/step - loss: 0.6865 - accuracy: 0.5500 - val_loss: 0.6890 - val_accuracy: 0.5347\n",
      "Epoch 90/200\n",
      "5255/5255 [==============================] - 180s 34ms/step - loss: 0.6864 - accuracy: 0.5505 - val_loss: 0.6903 - val_accuracy: 0.5340\n",
      "Epoch 91/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6863 - accuracy: 0.5495 - val_loss: 0.6916 - val_accuracy: 0.5365\n",
      "Epoch 92/200\n",
      "5255/5255 [==============================] - 180s 34ms/step - loss: 0.6863 - accuracy: 0.5499 - val_loss: 0.6902 - val_accuracy: 0.5377\n",
      "Epoch 93/200\n",
      "5255/5255 [==============================] - 181s 34ms/step - loss: 0.6864 - accuracy: 0.5504 - val_loss: 0.6903 - val_accuracy: 0.5384\n",
      "Epoch 94/200\n",
      "5255/5255 [==============================] - 183s 35ms/step - loss: 0.6867 - accuracy: 0.5491 - val_loss: 0.6892 - val_accuracy: 0.5407\n",
      "Epoch 95/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6865 - accuracy: 0.5494 - val_loss: 0.6899 - val_accuracy: 0.5402\n",
      "Epoch 96/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6864 - accuracy: 0.5498 - val_loss: 0.6893 - val_accuracy: 0.5348\n",
      "Epoch 97/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6865 - accuracy: 0.5495 - val_loss: 0.6904 - val_accuracy: 0.5335\n",
      "Epoch 98/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6865 - accuracy: 0.5489 - val_loss: 0.6889 - val_accuracy: 0.5371\n",
      "Epoch 99/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6865 - accuracy: 0.5486 - val_loss: 0.6907 - val_accuracy: 0.5351\n",
      "Epoch 100/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6867 - accuracy: 0.5496 - val_loss: 0.6893 - val_accuracy: 0.5369\n",
      "Epoch 101/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6867 - accuracy: 0.5476 - val_loss: 0.6899 - val_accuracy: 0.5352\n",
      "Epoch 102/200\n",
      "5255/5255 [==============================] - 184s 35ms/step - loss: 0.6869 - accuracy: 0.5471 - val_loss: 0.6913 - val_accuracy: 0.5351\n",
      "Epoch 103/200\n",
      "5255/5255 [==============================] - 185s 35ms/step - loss: 0.6866 - accuracy: 0.5497 - val_loss: 0.6898 - val_accuracy: 0.5343\n",
      "Epoch 104/200\n",
      "5255/5255 [==============================] - 187s 36ms/step - loss: 0.6870 - accuracy: 0.5486 - val_loss: 0.6922 - val_accuracy: 0.5370\n",
      "Epoch 105/200\n",
      "1619/5255 [========>.....................] - ETA: 2:04 - loss: 0.6863 - accuracy: 0.5501"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9eb469da1622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchkpt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\DAB200\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1190\u001b[0m                 _r=1):\n\u001b[0;32m   1191\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DAB200\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DAB200\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 913\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DAB200\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3030\u001b[0m       (graph_function,\n\u001b[0;32m   3031\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3032\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3033\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DAB200\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1954\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1955\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1956\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1957\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1958\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DAB200\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DAB200\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "\r\n",
    "joblib.dump(randomized_cvv, 'randomized_cvv.pkl')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['randomized_cvv.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "ypred_train = model.predict(x_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(15,6))\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.plot(np.arange(0,10),hist.history['val_accuracy'],label ='Validation')\r\n",
    "plt.plot(np.arange(0,10),hist.history['accuracy'],label ='Train')\r\n",
    "plt.title('Accuracy')\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.plot(np.arange(0,10),hist.history['val_loss'],label ='Validation')\r\n",
    "plt.plot(np.arange(0,10),hist.history['loss'],label ='Train')\r\n",
    "plt.tight_layout()\r\n",
    "plt.title('Loss')\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = make_subplots(rows=1,cols=2,x_title='Epochs',subplot_titles=('Accuracy','Loss'),horizontal_spacing=0.12 )\r\n",
    "\r\n",
    "fig.add_trace(trace=go.Scatter(x=np.arange(0,10),y=hist.history['val_accuracy'],name='Validation'),row=1,col=1 )\r\n",
    "fig.add_trace(trace =go.Scatter(x=np.arange(1,11),y=hist.history['accuracy'],name='Train'),row=1,col=1)\r\n",
    "\r\n",
    "fig.add_trace(trace =go.Scatter(x=np.arange(1,11),y=hist.history['val_loss'],name='Validation'),row=1,col=2)\r\n",
    "fig.add_trace(trace =go.Scatter(x=np.arange(1,11),y=hist.history['loss'],name='Train'),row=1,col=2)\r\n",
    "\r\n",
    "fig.update_layout(height=600, width=1200)\r\n",
    "fig.update_xaxes(range=(1,10))\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70eeece99edf9c6bc9f7c3070fd96f903c5d4a7abb3dc0dce1116ab3b071bfed"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('DAB200': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}